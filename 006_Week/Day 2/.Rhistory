timeseries = read.csv("/Users/suresh/Desktop/Insofe/006_Week/Day 2/DeliveryRate.csv",header =T)
str(timeseries)
timeseries$Date = as.Date(timeseries$Month,format="%d-%m-%Y")
minDate = min(timeseries$Date)
maxDate = max(timeseries$Date)
seq = data.frame(daterange = seq(minDate,maxDate,by="months"))
missing_timeseries = merge(seq,timeseries,by.x="daterange",by.y="Date",all.x=T)[,c('daterange','DeliveryRate')]
sum(is.na(missing_timeseries))
library(zoo)
missing_timeseries$DeliveryRate = na.locf(missing_timeseries$DeliveryRate)
final_data = missing_timeseries
train = final_data[as.numeric(rownames(final_data))<33,]
test = final_data[!as.numeric(rownames(final_data))<33,]
deliveryrate = ts(train$DeliveryRate,frequency = 12,start=c(2013,01))
par(mfrow = c(1,1))
plot(deliveryrate,type = 'l',lwd=3,col="blue",xlab="Month",ylab='DeliveryRate',main="Time Series Plot")
deliveryratedecomp <- decompose(deliveryrate)
plot(deliveryratedecomp)
par(mfrow=c(1,2))
acf(deliveryrate,lag=24)
pacf(deliveryrate,lag=24)
holtforecast <- HoltWinters(deliveryrate, beta=TRUE, gamma=TRUE, seasonal="additive")
head(holtforecast$fitted)
holtforecast_train = data.frame(holtforecast$fitted)
holtforecast$fitted$xhat
holtforecast$fitted[xhat]
holtforecast_train$xhat
deliveryrate
summary(holtforecast)
library(forecast)
deliveryrateforecast = forecast(holtforecast,h=4)
deliveryrateforecast
remove(list=ls())
read.csv("/Users/suresh/Desktop/Insofe/007_Week/Machine Learning Clustering/Cereals.csv",header=T)
cereals = read.csv("/Users/suresh/Desktop/Insofe/007_Week/Machine Learning Clustering/Cereals.csv",header=T)
str(cereals)
sum(is.na(cereals))
apply(cereals,2,sum(is.na()))
apply(cereals,2,FUN=function(x){sum(is.na(x))})
knnImputation(cereals,5)
library(DMwR)
knnImputation(cereals,5)
cereals = knnImputation(cereals,5)
cereals
apply(cereals,2,FUN=function(x){sum(is.na(x))})
library(vegan)
sapply(cereals,is.numeric())
sapply(cereals,is.numeric())
sapply(cereals,is.numeric)
decostand(cereals[,sapply(cereals,is.numeric)])
cereals[,sapply(cereals,is.numeric)]
decostand(cereals[,sapply(cereals,is.numeric)],'standardize')
cereals[,sapply(cereals,is.numeric)] = decostand(cereals[,sapply(cereals,is.numeric)],'standardize')
cereals
fit<-kmeans(mydata,centers=5)
str(cereals)
cereals = read.csv("/Users/suresh/Desktop/Insofe/007_Week/Machine Learning Clustering/Cereals.csv",header=T)
str(cereals)
table(cereals$shelf)
cereals$shelf = as.factor(cereals$shelf)
apply(cereals,2,FUN=function(x){sum(is.na(x))})
library(DMwR)
cereals = knnImputation(cereals,5)
library(vegan)
cereals[,sapply(cereals,is.numeric)] = decostand(cereals[,sapply(cereals,is.numeric)],'standardize')
fit<-kmeans(mydata,centers=5)
fit<-kmeans(cereals,centers=5)
apply(cereals,2,FUN=function(x){sum(is.na(x))})
kmeans(cereals,centers=5)
apply(cereals,2,FUN=function(x){sum(is.na(x))})
str(cereals)
fit<-kmeans(cereals,centers=5)
fit<-kmeans(cereals[,sapply(cereals,is.numeric)],centers=5)
cereals = read.csv("/Users/suresh/Desktop/Insofe/007_Week/Machine Learning Clustering/Cereals.csv",header=T)
str(cereals)
table(cereals$shelf)
apply(cereals,2,FUN=function(x){sum(is.na(x))})
library(DMwR)
cereals = knnImputation(cereals,5)
library(vegan)
cereals[,sapply(cereals,is.numeric)] = decostand(cereals[,sapply(cereals,is.numeric)],'standardize')
fit<-kmeans(cereals[,sapply(cereals,is.numeric)],centers=5)
summary(fit)
fit$withinss
fit$centers
fir$cluster
fit$cluster
("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F)
read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F)
remove(list=ls())
housing_data = read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F,sep="\t")
head(housing_data)
housing_data = read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F,sep="\s")
housing_data = read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F,sep=" ")
housing_data = read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F)
head(housing_data)
str(housing_data)
data.frame(colnames=c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV"))
nodata <- as.data.frame(setNames(replicate(14,numeric(0), simplify = F), c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")))
nodata
nodata <- housing_data
nodata
nodata <- as.data.frame(setNames(replicate(14,numeric(0), simplify = F), c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")))
nodata
nodata[,]
nodata[,] <- housing_data[,]
nodata[,]
housing_data[,]
replicate(5,numeric(0), simplify = F)
col.names(housing_data)
colnames(housing_data)
colnames(housing_data) = c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")
colnames(housing_data)
housing_data
head(housing_data)
str(housing_data)
sum(is.na(housing_data))
housing_data = read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F,na.strings="None")
head(housing_data)
sum(is.na(housing_data))
housing_data = read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F)
head(housing_data)
colnames(housing_data) = c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")
str(housing_data)
table(housing_data$CRIM)
table(housing_data$ZN)
table(housing_data$INDUS)
table(housing_data$CHAS)
housing_data$CHAS = as.factor(as.character(housing_data$CHAS))
table(housing_data$NOX)
table(housing_data$RM)
table(housing_data$DIS)
table(housing_data$RAD)
table(housing_data$TAX)
table(housing_data$PTRATIO)
table(housing_data$B)
table(housing_data$LSTAT)
table(housing_data$MEDV)
corrplot(cor(housing_data))
library("corrplot")
corrplot(cor(housing_data))
library("corrplot")
corrplot(cor(housing_data))
corrplot(cor(housing_data[,sapply(housing_data,isnumeric)])
corrplot(cor(housing_data[,sapply(housing_data,isnumeric)]))
housing_data[,sapply(housing_data,isnumeric)]
corrplot(cor(housing_data[,sapply(housing_data,is.numeric)]))
str(housing_data)
cor(housing_data$MEDV,housing_data$RM)
cor(housing_data$MEDV,housing_data$LSTAT)
set.seed(1234)
library(caret)
intrain = createDataPartition(y= housing_data$MEDV,p=0.80,list=FALSE)
train = housing_data[intrain,]
test  = housing_data[-intrain,]
linreg = lm(formula = MEDV~.,data=train)
summary(lireg)
summary(linreg)
predict_data_train = predict(linreg,newdata = train)
predict_data_test = predict(linreg,newdata = test)
regr.eval(train$MEDV,predict_data_train)
regr.eval(test$MEDV,predict_data_test)
par(msfrow=c(2,2))
plot(linreg)
par(msfrow=c(2,2))
plot(linreg)
par(msfrow=c(2,2))
par(mfrow=c(2,2))
plot(linreg)
stepout = stepAIC(linreg,direction= "both")
library(MASS)
stepout = stepAIC(linreg,direction= "both")
summary(linreg)
vif(stepout)
library(car)
vif(stepout)
predict_data_train = predict(stepout,newdata = train)
predict_data_test = predict(stepout,newdata = test)
regr.eval(train$MEDV,predict_data_train)
regr.eval(test$MEDV,predict_data_test)
housing_data
linreg_transform = lm(formula = log(MEDV)~.,data=train)
summary(linreg_transform)
plot(linreg_transform)
predict_data_train = predict(linreg_transform,newdata = train)
predict_data_test = predict(linreg_transform,newdata = test)
regr.eval(train$MEDV,predict_data_train)
regr.eval(test$MEDV,predict_data_test)
str(CRIM)
str(housing_data)
table(housing_data$CRIM)
max(housing_data$CRIM)
min(housing_data$CRIM)
mean(housing_data$CRIM)
median(housing_data$CRIM)
ifelse(housing_data$CRIM<5,"Low","High")
housing_data$Crimerate = ifelse(housing_data$CRIM<5,"Low","High")
housing_data$Crimerate = ifelse(housing_data$CRIM>=5&housing_data$CRIM<15,"Medium")
discretize(housing_data$CRIM, disc="equalwidth",nbins=3)
library(infotheo)
discretize(housing_data$CRIM, disc="equalwidth",nbins=3)
table(discretize(housing_data$CRIM, disc="equalwidth",nbins=3))
table(discretize(housing_data$CRIM, disc="equalfreq",nbins=3))
housing_data$Crimerate = ifelse(housing_data$CRIM<5,"Low","High")
housing_data$Crimerate = ifelse(housing_data$CRIM>=5&housing_data$CRIM<15,"Medium")
housing_data$Crimerate = ifelse(housing_data$CRIM>=5&housing_data$CRIM<15,"Medium","Low")
housing_data$Crimerate
table(discretize(housing_data$CRIM, disc="equalfreq",nbins=3))
table(discretize(housing_data$CRIM, disc="equalwidth",nbins=3))
summary(stepout)
housing_data$Crimerate = discretize(housing_data$CRIM, disc="equalwidth",nbins=3)
str(housing_data)
housing_data$Crimerate
housing_data$Crimerate = c(discretize(housing_data$CRIM, disc="equalwidth",nbins=3))
str(housing_data)
housing_data = read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/housing.data.txt",header=F)
colnames(housing_data) = c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")
housing_data$CHAS = as.factor(as.character(housing_data$CHAS))
housing_data$Crimerate<-NA
housing_data
for (i in 1:nrow(housing_data)){
if (housing_data$CRIM[i]>=15){
housing_data$Crimerate[i]="High"
}
if (housing_data$CRIM[i]>=5 & housing_data$CRIM[i]<15){
housing_data$Crimerate[i]="Medium"
}
else {
housing_data$Crimerate[i]="Low"
}
}
str(housing_data)
table(housing_data$Crimerate)
housing_data$Crimerate<-NA
for (i in 1:nrow(housing_data)){
if (housing_data$CRIM[i]>=15){
housing_data$Crimerate[i]="High"
}
else if (housing_data$CRIM[i]>=5 & housing_data$CRIM[i]<15){
housing_data$Crimerate[i]="Medium"
}
else {
housing_data$Crimerate[i]="Low"
}
}
table(housing_data$Crimerate)
linreg_transform = lm(formula = log(MEDV)~Crimerate + ZN + CHAS + NOX + RM + DIS + RAD +
TAX + PTRATIO + B + LSTAT,data=train)
train = housing_data[intrain,]
test  = housing_data[-intrain,]
linreg_transform = lm(formula = log(MEDV)~Crimerate + ZN + CHAS + NOX + RM + DIS + RAD +
TAX + PTRATIO + B + LSTAT,data=train)
predict_data_train = predict(linreg_transform,newdata = train)
predict_data_test = predict(linreg_transform,newdata = test)
regr.eval(train$MEDV,predict_data_train)
regr.eval(test$MEDV,predict_data_test)
summary(linreg_transform)
linreg_transform = lm(formula = log(MEDV)~Crimerate + ZN + CHAS + NOX + RM + DIS + RAD +
TAX + PTRATIO + B + LSTAT,data=train)
summary(linreg_transform)
predict_data_train = predict(linreg_transform,newdata = train)
predict_data_test = predict(linreg_transform,newdata = test)
regr.eval(train$MEDV,predict_data_train)
regr.eval(test$MEDV,predict_data_test)
predict_data_train = predict(linreg_transform,newdata = train)
predict_data_test = predict(linreg_transform,newdata = test)
regr.eval(train$MEDV,exp(predict_data_train))
regr.eval(test$MEDV,exp(predict_data_test))
plot(linreg)
remove(list=ls())
read.table("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/LogReg.RData")
load("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/LogReg.RData")
head(LogReg)
head(data)
data[,-which(colnames(customer)=="V1")]
prob = predict(LogReg,type="response")
prob_class = ifelse(prob > 0.5 ,1,0)
train_mat = table(train$Churned,prob_class)
summary(LogReg)
prob = predict(LogReg,type="response",newdata= data)
prob_class = ifelse(prob > 0.5 ,1,0)
head(data)
train_mat = table(data$V12,prob_class)
accuracy = sum(diag(data_mat))/sum(data_mat)
precision = data_mat[2,2]/sum(data_mat[,2])
recall = data_mat[2,2]/sum(data_mat[2,])
data_mat = table(data$V12,prob_class)
accuracy = sum(diag(data_mat))/sum(data_mat)
precision = data_mat[2,2]/sum(data_mat[,2])
recall = data_mat[2,2]/sum(data_mat[2,])
data_mat
c(accuracy,precision,recall)
predict_data = predict(LogReg,type="response",newdata= data)
prob_class = ifelse(predict_data > 0.5 ,1,0)
data_mat = table(data$V12,prob_class)
prob = prediction(predict_data,data$V12)
library(ROCR)
library(ggplot2)
prob = prediction(predict_data,data$V12)
tprfpr <- performance(prob, "tpr", "fpr")
par(mfrow=c(1,1))
plot(tprfpr)
tprfpr <- performance(prob, "tpr", "fpr")
par(mfrow=c(1,1))
plot(tprfpr)
plot(tprfpr,colorize=T,print.cutoffs.at=seq(0,1,by=0.1),text.adj=c(-0.2,1.7))
cutoffs = data.frame(cut=tprfpr@alpha.values[[1]],tpr = tprfpr@x.values[[1]],fpr = tprfpr@y.values[[1]])
cutoffs[order(cutoffs$tpr,decreasing = T),]
head(cutoffs[order(cutoffs$tpr,decreasing = T),])
head(subset(cutoffs, fpr < 0.6))
cutoffs[cutoffs$cut=0.1,]
cutoffs[cutoffs$cut==0.1,]
head(subset(cutoffs, fpr < 0.6))
head(subset(cutoffs, fpr < 0.2))
cutoffs[order(cutoffs$tpr,decreasing = T),]
head(cutoffs[order(cutoffs$tpr,decreasing = T),],1000)
head(cutoffs[order(cutoffs$tpr,decreasing = T),],100)
head(cutoffs[order(cutoffs$tpr,decreasing = T),])
head(subset(cutoffs, fpr < 0.8))
head(subset(cutoffs[order(cutoffs$tpr,decreasing = T),], fpr < 0.8))
head(subset(cutoffs[order(cutoffs$tpr,decreasing = T),], fpr < 0.6))
head(subset(cutoffs[order(cutoffs$tpr,decreasing = T),], cut < 0.1))
head(subset(cutoffs[order(cutoffs$tpr,decreasing = T),], cut > 0.1))
summary(tprfpr)
tprfpr
cutoffs = data.frame(cut=tprfpr@alpha.values[[1]],fpr = tprfpr@x.values[[1]],tpr = tprfpr@y.values[[1]])
head(subset(cutoffs[order(cutoffs$tpr,decreasing = T),], cut > 0.1))
head(subset(cutoffs[order(cutoffs$tpr,decreasing = T),], fpr <0.6))
library(Epi)
install.packages("Epi")
ROC(tprfpr)
predict_data = predict(LogReg,type="response",newdata= data)
prob_class = ifelse(predict_data > 0.1 ,1,0)
data_mat = table(data$V12,prob_class)
accuracy = sum(diag(data_mat))/sum(data_mat)
precision = data_mat[2,2]/sum(data_mat[,2])
recall = data_mat[2,2]/sum(data_mat[2,])
c(accuracy,precision,recall)
read.csv("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/TSData.csv",header=T)
data = read.csv("/Users/suresh/Desktop/Insofe/007_Week/002_GNQ/20170513_Batch 28_CSE 7202c_GNQ_Data/TSData.csv",header=T)
str(data)
data$Date = as.Date(data$Date,format="%d-%m-%Y")
minDate = min(data$Date)
maxDate = max(data$Date)
seq = data.frame(daterange = seq(minDate,maxDate,by="weeks"))
merge(seq,data,by.x="daterange",by.y="Date",all.x=T)
str(data)
missing_data = merge(seq,data,by.x="daterange",by.y="Date",all.x=T)[,c('daterange','Value')]
sum(is.na(missing_data))
library(zoo)
missing_data$Value = na.locf(missing_data$Value)
final_data = missing_data
final_data
train = final_data[as.numeric(rownames(final_data))<975,]
test = final_data[!as.numeric(rownames(final_data))<975,]
as.numeric(format(missing_data$daterange,"%Y.%W"))
missing_data$Week = as.numeric(format(missing_data$daterange,"%Y.%W"))
final_data = missing_data
train = final_data[as.numeric(rownames(final_data))<975,]
test = final_data[!as.numeric(rownames(final_data))<975,]
final_data[1,]
deliveryrate = ts(train$Value,frequency = 52,start=c(2010,00))
Value = ts(train$Value,frequency = 52,start=c(2010,00))
Value
par(mfrow=c(1,2))
acf(Value,lag=30)
pacf(Value,lag=30)
priceholtforecast <- HoltWinters(Value, beta=TRUE, gamma=TRUE, seasonal="additive")
holtforecastTrain <- data.frame(priceholtforecast$fitted)
library(forecast)
priceforecast <- forecast.HoltWinters(priceholtforecast, h=10)
holtforecastTrain
priceforecast
